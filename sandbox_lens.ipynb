{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a7eb1708070083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:41:00.041179Z",
     "start_time": "2024-09-01T10:40:49.783052Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tok/miniconda3/envs/lens/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers_wrapper import StableDiffusion3TextToImage, FluxTextToImage, StableDiffusion2TextToImage, StableDiffusionXLPipelineTextToImage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c4d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "main_output_path = 'output'\n",
    "model_name = 'flux-schnell' # 'sd2' or 'sd3' or 'sdxl' or 'flux-dev' or 'flux-schnell'\n",
    "max_sequence_lengths ={\n",
    "    'sd2': 77,\n",
    "    'sd3': 77,\n",
    "    'sdxl': 256,\n",
    "    'flux-dev': 512,\n",
    "    'flux-schnell': 256\n",
    "}\n",
    "max_sequence_length = max_sequence_lengths[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b9447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.32it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'flux' in model_name:\n",
    "    ckpt_dir = ''\n",
    "    model_class = FluxTextToImage(model_name=model_name, ckpt_dir=ckpt_dir, num_images=num_images, \n",
    "                                  max_sequence_length=max_sequence_length)\n",
    "elif model_name == 'sd2':\n",
    "    model_name = model_name\n",
    "    ckpt_dir = ''\n",
    "    model_class = StableDiffusion2TextToImage(model_name=model_name, ckpt_dir=ckpt_dir, num_images=num_images)\n",
    "elif model_name == 'sd3':\n",
    "    model_name = model_name\n",
    "    ckpt_dir = ''\n",
    "    model_class = StableDiffusion3TextToImage(model_name=model_name, ckpt_dir=ckpt_dir, num_images=num_images, \n",
    "                                              max_sequence_length=max_sequence_length) \n",
    "elif model_name == 'sdxl':\n",
    "    model_name = model_name\n",
    "    ckpt_dir = ''\n",
    "    model_class = StableDiffusionXLPipelineTextToImage(model_name=model_name, ckpt_dir=ckpt_dir, num_images=num_images, \n",
    "                                                       max_sequence_length=max_sequence_length)\n",
    "else:\n",
    "    raise ValueError(\"Model name not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379899c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 kids\n",
      "1 playing\n",
      "2 in\n",
      "3 the\n",
      "4 play\n",
      "5 ground\n",
      "6 </s>\n"
     ]
    }
   ],
   "source": [
    "skip_layers = 0\n",
    "return_grids = True\n",
    "# full is the full range of tokens with padas - remove if you dont need images from this range\n",
    "# tokens is the range of tokens without padas - remove if you dont need images from this range\n",
    "# specific_tokens - if you want images from specific tokens, \n",
    "ranges_to_keep = ['full', 'tokens', 'pads', 'specific_token_idx_to_keep_per_prompt']\n",
    "\n",
    "prompts = [\n",
    "    'kids playing in the play ground',\n",
    "]\n",
    "\n",
    "tokenizers = model_class.get_tokenizers()\n",
    "tokenizer_3 = tokenizers['tokenizer_2']\n",
    "tokenized_prompts = [tokenizer_3.encode(prompt) for prompt in prompts]\n",
    "for prompt in tokenized_prompts:\n",
    "    for subtoken_idx, subtoken in enumerate(tokenized_prompts[0]):\n",
    "        print(subtoken_idx, tokenizer_3.decode(subtoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0546e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# each list would results in generating images fro, the tokens in the list only. If you want to generate image from each prompt, pass a list for each prompt\n",
    "# specific_tokens_per_prompt = [\n",
    "#     ['kids', 'in']\n",
    "# ]\n",
    "\n",
    "specific_token_idx_to_keep_per_prompt = [\n",
    "    [5]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38fef12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating images for prompt: kids playing in the play ground\n",
      "Validated skip_layers: [0, 0]\n",
      "ranges_to_keep ['full', 'tokens', 'pads', 'specific_token_idx_to_keep_per_prompt']\n",
      "Getting range for full\n",
      "Getting range for tokens\n",
      "Getting range for pads\n",
      "Getting range for specific_token_idx_to_keep_per_prompt\n",
      "st_ground: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "text_ids shape [tokenizer - CLIP]:  torch.Size([1, 77])\n",
      "Original CLIP pooling code\n",
      "text_ids shape [tokenizer_2 - T5]:  torch.Size([1, 256])\n",
      "Skiping num layers:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image grid for full already exists, skipping\n",
      "text_ids shape [tokenizer - CLIP]:  torch.Size([1, 77])\n",
      "Original CLIP pooling code\n",
      "text_ids shape [tokenizer_2 - T5]:  torch.Size([1, 256])\n",
      "Skiping num layers:  0\n",
      "skip_tokens (T5):  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "replacing 7 replacing 8 replacing 9 replacing 10 replacing 11 replacing 12 replacing 13 replacing 14 replacing 15 replacing 16 replacing 17 replacing 18 replacing 19 replacing 20 replacing 21 replacing 22 replacing 23 replacing 24 replacing 25 replacing 26 replacing 27 replacing 28 replacing 29 replacing 30 replacing 31 replacing 32 replacing 33 replacing 34 replacing 35 replacing 36 replacing 37 replacing 38 replacing 39 replacing 40 replacing 41 replacing 42 replacing 43 replacing 44 replacing 45 replacing 46 replacing 47 replacing 48 replacing 49 replacing 50 replacing 51 replacing 52 replacing 53 replacing 54 replacing 55 replacing 56 replacing 57 replacing 58 replacing 59 replacing 60 replacing 61 replacing 62 replacing 63 replacing 64 replacing 65 replacing 66 replacing 67 replacing 68 replacing 69 replacing 70 replacing 71 replacing 72 replacing 73 replacing 74 replacing 75 replacing 76 replacing 77 replacing 78 replacing 79 replacing 80 replacing 81 replacing 82 replacing 83 replacing 84 replacing 85 replacing 86 replacing 87 replacing 88 replacing 89 replacing 90 replacing 91 replacing 92 replacing 93 replacing 94 replacing 95 replacing 96 replacing 97 replacing 98 replacing 99 replacing 100 replacing 101 replacing 102 replacing 103 replacing 104 replacing 105 replacing 106 replacing 107 replacing 108 replacing 109 replacing 110 replacing 111 replacing 112 replacing 113 replacing 114 replacing 115 replacing 116 replacing 117 replacing 118 replacing 119 replacing 120 replacing 121 replacing 122 replacing 123 replacing 124 replacing 125 replacing 126 replacing 127 replacing 128 replacing 129 replacing 130 replacing 131 replacing 132 replacing 133 replacing 134 replacing 135 replacing 136 replacing 137 replacing 138 replacing 139 replacing 140 replacing 141 replacing 142 replacing 143 replacing 144 replacing 145 replacing 146 replacing 147 replacing 148 replacing 149 replacing 150 replacing 151 replacing 152 replacing 153 replacing 154 replacing 155 replacing 156 replacing 157 replacing 158 replacing 159 replacing 160 replacing 161 replacing 162 replacing 163 replacing 164 replacing 165 replacing 166 replacing 167 replacing 168 replacing 169 replacing 170 replacing 171 replacing 172 replacing 173 replacing 174 replacing 175 replacing 176 replacing 177 replacing 178 replacing 179 replacing 180 replacing 181 replacing 182 replacing 183 replacing 184 replacing 185 replacing 186 replacing 187 replacing 188 replacing 189 replacing 190 replacing 191 replacing 192 replacing 193 replacing 194 replacing 195 replacing 196 replacing 197 replacing 198 replacing 199 replacing 200 replacing 201 replacing 202 replacing 203 replacing 204 replacing 205 replacing 206 replacing 207 replacing 208 replacing 209 replacing 210 replacing 211 replacing 212 replacing 213 replacing 214 replacing 215 replacing 216 replacing 217 replacing 218 replacing 219 replacing 220 replacing 221 replacing 222 replacing 223 replacing 224 replacing 225 replacing 226 replacing 227 replacing 228 replacing 229 replacing 230 replacing 231 replacing 232 replacing 233 replacing 234 replacing 235 replacing 236 replacing 237 replacing 238 replacing 239 replacing 240 replacing 241 replacing 242 replacing 243 replacing 244 replacing 245 replacing 246 replacing 247 replacing 248 replacing 249 replacing 250 replacing 251 replacing 252 replacing 253 replacing 254 replacing 255 Keep token_idx:  0 Keep token_idx:  1 Keep token_idx:  2 Keep token_idx:  3 Keep token_idx:  4 Keep token_idx:  5 Keep token_idx:  6 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image grid for tokens already exists, skipping\n",
      "text_ids shape [tokenizer - CLIP]:  torch.Size([1, 77])\n",
      "Original CLIP pooling code\n",
      "text_ids shape [tokenizer_2 - T5]:  torch.Size([1, 256])\n",
      "Skiping num layers:  0\n",
      "skip_tokens (T5):  [0, 1, 2, 3, 4, 5, 6]\n",
      "replacing 0 replacing 1 replacing 2 replacing 3 replacing 4 replacing 5 replacing 6 Keep token_idx:  7 Keep token_idx:  8 Keep token_idx:  9 Keep token_idx:  10 Keep token_idx:  11 Keep token_idx:  12 Keep token_idx:  13 Keep token_idx:  14 Keep token_idx:  15 Keep token_idx:  16 Keep token_idx:  17 Keep token_idx:  18 Keep token_idx:  19 Keep token_idx:  20 Keep token_idx:  21 Keep token_idx:  22 Keep token_idx:  23 Keep token_idx:  24 Keep token_idx:  25 Keep token_idx:  26 Keep token_idx:  27 Keep token_idx:  28 Keep token_idx:  29 Keep token_idx:  30 Keep token_idx:  31 Keep token_idx:  32 Keep token_idx:  33 Keep token_idx:  34 Keep token_idx:  35 Keep token_idx:  36 Keep token_idx:  37 Keep token_idx:  38 Keep token_idx:  39 Keep token_idx:  40 Keep token_idx:  41 Keep token_idx:  42 Keep token_idx:  43 Keep token_idx:  44 Keep token_idx:  45 Keep token_idx:  46 Keep token_idx:  47 Keep token_idx:  48 Keep token_idx:  49 Keep token_idx:  50 Keep token_idx:  51 Keep token_idx:  52 Keep token_idx:  53 Keep token_idx:  54 Keep token_idx:  55 Keep token_idx:  56 Keep token_idx:  57 Keep token_idx:  58 Keep token_idx:  59 Keep token_idx:  60 Keep token_idx:  61 Keep token_idx:  62 Keep token_idx:  63 Keep token_idx:  64 Keep token_idx:  65 Keep token_idx:  66 Keep token_idx:  67 Keep token_idx:  68 Keep token_idx:  69 Keep token_idx:  70 Keep token_idx:  71 Keep token_idx:  72 Keep token_idx:  73 Keep token_idx:  74 Keep token_idx:  75 Keep token_idx:  76 Keep token_idx:  77 Keep token_idx:  78 Keep token_idx:  79 Keep token_idx:  80 Keep token_idx:  81 Keep token_idx:  82 Keep token_idx:  83 Keep token_idx:  84 Keep token_idx:  85 Keep token_idx:  86 Keep token_idx:  87 Keep token_idx:  88 Keep token_idx:  89 Keep token_idx:  90 Keep token_idx:  91 Keep token_idx:  92 Keep token_idx:  93 Keep token_idx:  94 Keep token_idx:  95 Keep token_idx:  96 Keep token_idx:  97 Keep token_idx:  98 Keep token_idx:  99 Keep token_idx:  100 Keep token_idx:  101 Keep token_idx:  102 Keep token_idx:  103 Keep token_idx:  104 Keep token_idx:  105 Keep token_idx:  106 Keep token_idx:  107 Keep token_idx:  108 Keep token_idx:  109 Keep token_idx:  110 Keep token_idx:  111 Keep token_idx:  112 Keep token_idx:  113 Keep token_idx:  114 Keep token_idx:  115 Keep token_idx:  116 Keep token_idx:  117 Keep token_idx:  118 Keep token_idx:  119 Keep token_idx:  120 Keep token_idx:  121 Keep token_idx:  122 Keep token_idx:  123 Keep token_idx:  124 Keep token_idx:  125 Keep token_idx:  126 Keep token_idx:  127 Keep token_idx:  128 Keep token_idx:  129 Keep token_idx:  130 Keep token_idx:  131 Keep token_idx:  132 Keep token_idx:  133 Keep token_idx:  134 Keep token_idx:  135 Keep token_idx:  136 Keep token_idx:  137 Keep token_idx:  138 Keep token_idx:  139 Keep token_idx:  140 Keep token_idx:  141 Keep token_idx:  142 Keep token_idx:  143 Keep token_idx:  144 Keep token_idx:  145 Keep token_idx:  146 Keep token_idx:  147 Keep token_idx:  148 Keep token_idx:  149 Keep token_idx:  150 Keep token_idx:  151 Keep token_idx:  152 Keep token_idx:  153 Keep token_idx:  154 Keep token_idx:  155 Keep token_idx:  156 Keep token_idx:  157 Keep token_idx:  158 Keep token_idx:  159 Keep token_idx:  160 Keep token_idx:  161 Keep token_idx:  162 Keep token_idx:  163 Keep token_idx:  164 Keep token_idx:  165 Keep token_idx:  166 Keep token_idx:  167 Keep token_idx:  168 Keep token_idx:  169 Keep token_idx:  170 Keep token_idx:  171 Keep token_idx:  172 Keep token_idx:  173 Keep token_idx:  174 Keep token_idx:  175 Keep token_idx:  176 Keep token_idx:  177 Keep token_idx:  178 Keep token_idx:  179 Keep token_idx:  180 Keep token_idx:  181 Keep token_idx:  182 Keep token_idx:  183 Keep token_idx:  184 Keep token_idx:  185 Keep token_idx:  186 Keep token_idx:  187 Keep token_idx:  188 Keep token_idx:  189 Keep token_idx:  190 Keep token_idx:  191 Keep token_idx:  192 Keep token_idx:  193 Keep token_idx:  194 Keep token_idx:  195 Keep token_idx:  196 Keep token_idx:  197 Keep token_idx:  198 Keep token_idx:  199 Keep token_idx:  200 Keep token_idx:  201 Keep token_idx:  202 Keep token_idx:  203 Keep token_idx:  204 Keep token_idx:  205 Keep token_idx:  206 Keep token_idx:  207 Keep token_idx:  208 Keep token_idx:  209 Keep token_idx:  210 Keep token_idx:  211 Keep token_idx:  212 Keep token_idx:  213 Keep token_idx:  214 Keep token_idx:  215 Keep token_idx:  216 Keep token_idx:  217 Keep token_idx:  218 Keep token_idx:  219 Keep token_idx:  220 Keep token_idx:  221 Keep token_idx:  222 Keep token_idx:  223 Keep token_idx:  224 Keep token_idx:  225 Keep token_idx:  226 Keep token_idx:  227 Keep token_idx:  228 Keep token_idx:  229 Keep token_idx:  230 Keep token_idx:  231 Keep token_idx:  232 Keep token_idx:  233 Keep token_idx:  234 Keep token_idx:  235 Keep token_idx:  236 Keep token_idx:  237 Keep token_idx:  238 Keep token_idx:  239 Keep token_idx:  240 Keep token_idx:  241 Keep token_idx:  242 Keep token_idx:  243 Keep token_idx:  244 Keep token_idx:  245 Keep token_idx:  246 Keep token_idx:  247 Keep token_idx:  248 Keep token_idx:  249 Keep token_idx:  250 Keep token_idx:  251 Keep token_idx:  252 Keep token_idx:  253 Keep token_idx:  254 Keep token_idx:  255 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image grid for pads already exists, skipping\n",
      "text_ids shape [tokenizer - CLIP]:  torch.Size([1, 77])\n",
      "Original CLIP pooling code\n",
      "text_ids shape [tokenizer_2 - T5]:  torch.Size([1, 256])\n",
      "Skiping num layers:  0\n",
      "skip_tokens (T5):  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "replacing 0 replacing 1 replacing 2 replacing 3 replacing 4 replacing 6 replacing 7 replacing 8 replacing 9 replacing 10 replacing 11 replacing 12 replacing 13 replacing 14 replacing 15 replacing 16 replacing 17 replacing 18 replacing 19 replacing 20 replacing 21 replacing 22 replacing 23 replacing 24 replacing 25 replacing 26 replacing 27 replacing 28 replacing 29 replacing 30 replacing 31 replacing 32 replacing 33 replacing 34 replacing 35 replacing 36 replacing 37 replacing 38 replacing 39 replacing 40 replacing 41 replacing 42 replacing 43 replacing 44 replacing 45 replacing 46 replacing 47 replacing 48 replacing 49 replacing 50 replacing 51 replacing 52 replacing 53 replacing 54 replacing 55 replacing 56 replacing 57 replacing 58 replacing 59 replacing 60 replacing 61 replacing 62 replacing 63 replacing 64 replacing 65 replacing 66 replacing 67 replacing 68 replacing 69 replacing 70 replacing 71 replacing 72 replacing 73 replacing 74 replacing 75 replacing 76 replacing 77 replacing 78 replacing 79 replacing 80 replacing 81 replacing 82 replacing 83 replacing 84 replacing 85 replacing 86 replacing 87 replacing 88 replacing 89 replacing 90 replacing 91 replacing 92 replacing 93 replacing 94 replacing 95 replacing 96 replacing 97 replacing 98 replacing 99 replacing 100 replacing 101 replacing 102 replacing 103 replacing 104 replacing 105 replacing 106 replacing 107 replacing 108 replacing 109 replacing 110 replacing 111 replacing 112 replacing 113 replacing 114 replacing 115 replacing 116 replacing 117 replacing 118 replacing 119 replacing 120 replacing 121 replacing 122 replacing 123 replacing 124 replacing 125 replacing 126 replacing 127 replacing 128 replacing 129 replacing 130 replacing 131 replacing 132 replacing 133 replacing 134 replacing 135 replacing 136 replacing 137 replacing 138 replacing 139 replacing 140 replacing 141 replacing 142 replacing 143 replacing 144 replacing 145 replacing 146 replacing 147 replacing 148 replacing 149 replacing 150 replacing 151 replacing 152 replacing 153 replacing 154 replacing 155 replacing 156 replacing 157 replacing 158 replacing 159 replacing 160 replacing 161 replacing 162 replacing 163 replacing 164 replacing 165 replacing 166 replacing 167 replacing 168 replacing 169 replacing 170 replacing 171 replacing 172 replacing 173 replacing 174 replacing 175 replacing 176 replacing 177 replacing 178 replacing 179 replacing 180 replacing 181 replacing 182 replacing 183 replacing 184 replacing 185 replacing 186 replacing 187 replacing 188 replacing 189 replacing 190 replacing 191 replacing 192 replacing 193 replacing 194 replacing 195 replacing 196 replacing 197 replacing 198 replacing 199 replacing 200 replacing 201 replacing 202 replacing 203 replacing 204 replacing 205 replacing 206 replacing 207 replacing 208 replacing 209 replacing 210 replacing 211 replacing 212 replacing 213 replacing 214 replacing 215 replacing 216 replacing 217 replacing 218 replacing 219 replacing 220 replacing 221 replacing 222 replacing 223 replacing 224 replacing 225 replacing 226 replacing 227 replacing 228 replacing 229 replacing 230 replacing 231 replacing 232 replacing 233 replacing 234 replacing 235 replacing 236 replacing 237 replacing 238 replacing 239 replacing 240 replacing 241 replacing 242 replacing 243 replacing 244 replacing 245 replacing 246 replacing 247 replacing 248 replacing 249 replacing 250 replacing 251 replacing 252 replacing 253 replacing 254 replacing 255 Keep token_idx:  5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image grid for 4 images\n",
      "Image grid saved to output/flux-schnell/kids playing in the play ground/st_ground/st_ground_[0, 0].png\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "for prompt, specific_token_idx in zip(prompts, specific_token_idx_to_keep_per_prompt):\n",
    "    print(\"generating images for prompt:\", prompt)         \n",
    "    output_path = f'{main_output_path}/{model_name}/{prompt}'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    if 'flux' in model_name:\n",
    "        grids = model_class.forward(prompt, num_images=model_class.num_images, \n",
    "                    output_path=output_path, \n",
    "                    save_grid=True, \n",
    "                    save_per_image=False, \n",
    "                    return_grids=return_grids,\n",
    "                    skip_layers=skip_layers,\n",
    "                    ranges_to_keep=ranges_to_keep,\n",
    "                    specific_token_idx_to_keep_per_prompt=specific_token_idx,\n",
    "                    # specific_tokens=specific_tokens,\n",
    "                    # pad_encoders=pad_encoders,\n",
    "                    # zero_paddings=False, \n",
    "                    # replace_with_pads=True,\n",
    "                    # turn_attention_off=False,\n",
    "                    )\n",
    "    else:\n",
    "        pad_encoders = [1,2] # 1,2 for clips, 3 for T5\n",
    "        grids = model_class.forward(prompt, num_images=model_class.num_images, \n",
    "                            output_path=output_path, \n",
    "                            save_grid=True, \n",
    "                            save_per_image=False, \n",
    "                            return_grids=return_grids,\n",
    "                            skip_layers=skip_layers,\n",
    "                            ranges_to_keep=ranges_to_keep,\n",
    "                            specific_token_idx_to_keep_per_prompt=specific_token_idx_to_keep_per_prompt,\n",
    "                            # specific_tokens=specific_tokens,\n",
    "                            pad_encoders=pad_encoders,\n",
    "                            # zero_paddings=False, \n",
    "                            # replace_with_pads=True,\n",
    "                            # turn_attention_off=False,\n",
    "                            )\n",
    "    # print(\"done generating images for prompt:\", prompt)\n",
    "    # for grid in grids:\n",
    "    #     grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70af03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760480b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
